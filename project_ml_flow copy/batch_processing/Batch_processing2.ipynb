{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Simple implementation for processing  129 MB file\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.combine import SMOTETomek\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Your SAS URL\n",
        "#sas_url = \"https://vestiairedata1.blob.core.windows.net/vestiairecontainer?sp=racwdli&st=2025-04-27T19:48:10Z&se=2025-06-07T03:48:10Z&spr=https&sv=2024-11-04&sr=c&sig=e%2BAbOapdiakxuDf9%2BXw8%2BwLZp51dZqLdSg%2FmjmxPpCI%3D\"\n",
        "# Update your SAS URL to point to the specific file\n",
        "sas_url = \"https://vestiairedata1.blob.core.windows.net/vestiairecontainer/cleaned_data.parquet?sp=racwdli&st=2025-05-05T01:36:02Z&se=2025-05-15T09:36:02Z&spr=https&sv=2024-11-04&sr=c&sig=n32zqbxDDp%2FRehKIM88ScyLA87jJsJ%2FEmjloSPJ%2BIJc%3D\"\n",
        "\n",
        "def get_important_features():\n",
        "    \"\"\"Return the list of important features for modeling.\"\"\"\n",
        "    return [\n",
        "        'seller_price', 'seller_badge_encoded', 'should_be_gone', 'seller_pass_rate',\n",
        "        'price_to_earning_ratio', 'seller_products_sold', 'price_per_like', 'brand_id',\n",
        "        'product_type', 'product_material', 'product_like_count', 'seller_num_products_listed',\n",
        "        'seller_community_rank', 'seller_activity_ratio', 'product_color_encoded',\n",
        "        'seller_num_followers', 'available', 'seller_country', 'in_stock',\n",
        "        'product_season_encoded', 'usually_ships_within_encoded', 'product_condition_encoded',\n",
        "        'warehouse_name_encoded'\n",
        "    ]\n",
        "\n",
        "# Step 1: Read the data\n",
        "import requests\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow.fs\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"Reading data from SAS URL...\")\n",
        "\n",
        "# If the SAS URL points to a container, you need to specify a file within it\n",
        "# SAS URLs can be tricky, let's try multiple approaches:\n",
        "\n",
        "# Method 1: Try direct read with storage_options\n",
        "try:\n",
        "    df = pd.read_parquet(sas_url)\n",
        "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "except:\n",
        "    # Method 2: Try with requests\n",
        "    try:\n",
        "        response = requests.get(sas_url)\n",
        "        response.raise_for_status()\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "    except Exception as e:\n",
        "        # Method 3: Try with azure storage specific options\n",
        "        try:\n",
        "            from azure.storage.blob import BlobServiceClient\n",
        "            \n",
        "            # Extract account_url and sas_token from the URL\n",
        "            parts = sas_url.split('?')\n",
        "            if len(parts) == 2:\n",
        "                blob_url = parts[0]\n",
        "                sas_token = '?' + parts[1]\n",
        "                \n",
        "                # Try to list blobs if it's a container URL\n",
        "                print(\"Attempting to list files in the container...\")\n",
        "                blob_service_client = BlobServiceClient(blob_url.split('/')[2], sas_token)\n",
        "                container_name = blob_url.split('/')[-1]\n",
        "                container_client = blob_service_client.get_container_client(container_name)\n",
        "                \n",
        "                # List first 5 files\n",
        "                blob_list = container_client.list_blobs()\n",
        "                files = []\n",
        "                for blob in blob_list:\n",
        "                    files.append(blob.name)\n",
        "                    if len(files) >= 5:\n",
        "                        break\n",
        "                \n",
        "                print(\"First 5 files in container:\")\n",
        "                for f in files:\n",
        "                    print(f\"  - {f}\")\n",
        "                \n",
        "                # Try reading the first parquet file\n",
        "                if files:\n",
        "                    first_file = files[0]\n",
        "                    full_url = f\"{blob_url}/{first_file}{sas_token}\"\n",
        "                    df = pd.read_parquet(full_url)\n",
        "                    print(f\"Data loaded successfully from {first_file}. Shape: {df.shape}\")\n",
        "                    print(f\"Columns: {df.columns.tolist()}\")\n",
        "                else:\n",
        "                    print(\"No files found in container\")\n",
        "            else:\n",
        "                print(\"Invalid SAS URL format\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Error: {e2}\")\n",
        "            print(\"Please provide a valid SAS URL to a specific parquet file\")\n",
        "            # Try to give more context about the error\n",
        "            print(\"\\nYour SAS URL should point to:\")\n",
        "            print(\"1. A specific parquet file (preferred)\")\n",
        "            print(\"2. Or a container with read permissions\")\n",
        "\n",
        "# Step 2: Check if the data has the target variable (sold)\n",
        "if 'sold' in df.columns:\n",
        "    print(\"\\nTraining a model...\")\n",
        "    \n",
        "    # Get features\n",
        "    features = get_important_features()\n",
        "    \n",
        "    # Check which features exist\n",
        "    available_features = [f for f in features if f in df.columns]\n",
        "    missing_features = [f for f in features if f not in df.columns]\n",
        "    \n",
        "    print(f\"Available features: {len(available_features)}/{len(features)}\")\n",
        "    if missing_features:\n",
        "        print(f\"Missing features: {missing_features}\")\n",
        "    \n",
        "    # Prepare data with available features\n",
        "    X = df[available_features]\n",
        "    y = df['sold']\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Apply SMOTE-Tomek\n",
        "    smote_tomek = SMOTETomek(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
        "    \n",
        "    # Train XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        eval_metric='auc',\n",
        "        random_state=42,\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    \n",
        "    # Evaluate\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    joblib.dump(model, 'models/model.pkl')\n",
        "    joblib.dump(available_features, 'models/feature_names.pkl')\n",
        "    print(\"Model saved to 'models/' directory\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\nNo 'sold' column found. Loading model for prediction...\")\n",
        "    \n",
        "    # Load model if it exists\n",
        "    if os.path.exists('models/model.pkl'):\n",
        "        model = joblib.load('models/model.pkl')\n",
        "        feature_names = joblib.load('models/feature_names.pkl')\n",
        "        \n",
        "        # Prepare features\n",
        "        X = df[feature_names]\n",
        "        \n",
        "        # Make predictions\n",
        "        print(\"Making predictions...\")\n",
        "        predictions = model.predict_proba(X)[:, 1]\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results_df = pd.DataFrame({\n",
        "            'id': df.index,\n",
        "            'prediction': predictions\n",
        "        })\n",
        "        \n",
        "        # Save results\n",
        "        results_df.to_csv('predictions.csv', index=False)\n",
        "        print(f\"Predictions saved to 'predictions.csv'\")\n",
        "        print(f\"Number of predictions: {len(results_df)}\")\n",
        "        print(\"\\nSample predictions:\")\n",
        "        print(results_df.head())\n",
        "        \n",
        "    else:\n",
        "        print(\"No model found. Please train a model first.\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'null/Users/berly.biju'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull/Users/berly.biju\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'null/Users/berly.biju'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1746410359122
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}