{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Processing with Azure"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install imblearn\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'null/Users/berly.biju'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull/Users/berly.biju\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'null/Users/berly.biju'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Authenticate and initialize the MLClient\n",
        "credential = DefaultAzureCredential()  # Use your preferred authentication method\n",
        "ml_client = MLClient(credential, subscription_id=\"2d5dc9ea-1c38-4712-9d56-d8f22e4900ea\", resource_group_name=\"VestiaireCollective_Demand_Forecasting\", workspace_name=\"Vestiaire_ML_workspace\")\n",
        "\n",
        "# Now, you can use ml_client to interact with Azure ML\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO:azure.identity._credentials.environment:No environment configuration found.\nINFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\nWARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\nWARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\nWARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1746406983184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize ML client\n",
        "ml_client = MLClient(\n",
        "    credential=DefaultAzureCredential(),\n",
        "    subscription_id=\"2d5dc9ea-1c38-4712-9d56-d8f22e4900ea\",\n",
        "    resource_group_name=\"VestiaireCollective_Demand_Forecasting\",\n",
        "    workspace_name=\"Vestiaire_ML_workspace\"\n",
        ")\n",
        "\n",
        "# First create the batch endpoint\n",
        "endpoint_name = \"price-elasticity-batch\"\n",
        "endpoint = BatchEndpoint(\n",
        "    name=endpoint_name,\n",
        "    description=\"Batch endpoint for price elasticity prediction\",\n",
        "    auth_mode=\"aad_token\"\n",
        ")\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "\n",
        "# Then create the deployment\n",
        "deployment = BatchDeployment(\n",
        "    name=\"xgboost-deployment\",\n",
        "    description=\"XGBoost batch deployment\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=\"azureml:price-elasticity-model@latest\",  # Replace with your registered model\n",
        "    code_configuration={\n",
        "        \"code\": \"./src\",\n",
        "        \"scoring_script\": \"score.py\"\n",
        "    },\n",
        "    environment={\n",
        "        \"conda_file\": \"./environment.yml\",\n",
        "        \"docker\": {\n",
        "            \"image\": \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04\"\n",
        "        }\n",
        "    },\n",
        "    compute=\"berly-insy-cluster\",  # Make sure this cluster exists\n",
        "    instance_count=2,\n",
        "    max_concurrency_per_instance=2,\n",
        "    mini_batch_size=1000,\n",
        "    output_action=\"append_row\",\n",
        "    output_file_name=\"predictions.csv\",\n",
        "    retry_settings={\n",
        "        \"max_retries\": 3,\n",
        "        \"timeout\": 300\n",
        "    }\n",
        ")\n",
        "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO:azure.identity._credentials.environment:No environment configuration found.\nINFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\nWARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\nWARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\nWARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nINFO:azure.identity._credentials.chained:DefaultAzureCredential acquired a token from ManagedIdentityCredential\nINFO:azure.identity._internal.msal_managed_identity_client:AzureMLCredential.get_token_info succeeded\nINFO:azure.identity._internal.decorators:ManagedIdentityCredential.get_token_info succeeded\nINFO:azure.identity._credentials.default:DefaultAzureCredential acquired a token from ManagedIdentityCredential\n"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'code'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 49\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Then create the deployment\u001b[39;00m\n\u001b[1;32m     23\u001b[0m deployment \u001b[38;5;241m=\u001b[39m BatchDeployment(\n\u001b[1;32m     24\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost-deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost batch deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     }\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 49\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(ACTIVITY_SPAN):\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    286\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    287\u001b[0m         ):\n\u001b[0;32m--> 288\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_batch_deployment_operations.py:125\u001b[0m, in \u001b[0;36mBatchDeploymentOperations.begin_create_or_update\u001b[0;34m(self, deployment, skip_script_validation, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@monitor_with_activity\u001b[39m(ops_logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatchDeployment.BeginCreateOrUpdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, ActivityType\u001b[38;5;241m.\u001b[39mPUBLICAPI)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbegin_create_or_update\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LROPoller[DeploymentType]:\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create or update a batch deployment.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    :param deployment: The deployment entity.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m            :caption: Create example.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m skip_script_validation\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(deployment, PipelineComponentBatchDeployment)\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m deployment\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m deployment\u001b[38;5;241m.\u001b[39mcode_configuration  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(ARM_ID_PREFIX)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(AMLVersionedArmId\u001b[38;5;241m.\u001b[39mREGEX_PATTERN, deployment\u001b[38;5;241m.\u001b[39mcode_configuration\u001b[38;5;241m.\u001b[39mcode)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     ):\n\u001b[1;32m    128\u001b[0m         validate_scoring_script(deployment)\n\u001b[1;32m    129\u001b[0m     module_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m exists\u001b[39m\u001b[38;5;124m\"\u001b[39m, deployment\u001b[38;5;241m.\u001b[39mendpoint_name)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'code'"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1746407273627
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple implementation for processing y 129 MB file\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.combine import SMOTETomek\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Your SAS URL\n",
        "#sas_url = \"https://vestiairedata1.blob.core.windows.net/vestiairecontainer?sp=racwdli&st=2025-04-27T19:48:10Z&se=2025-06-07T03:48:10Z&spr=https&sv=2024-11-04&sr=c&sig=e%2BAbOapdiakxuDf9%2BXw8%2BwLZp51dZqLdSg%2FmjmxPpCI%3D\"\n",
        "# Update your SAS URL to point to the specific file\n",
        "sas_url = \"https://vestiairedata1.blob.core.windows.net/vestiairecontainer/cleaned_data.parquet?sp=racwdli&st=2025-05-05T01:36:02Z&se=2025-05-15T09:36:02Z&spr=https&sv=2024-11-04&sr=c&sig=n32zqbxDDp%2FRehKIM88ScyLA87jJsJ%2FEmjloSPJ%2BIJc%3D\"\n",
        "\n",
        "def get_important_features():\n",
        "    \"\"\"Return the list of important features for modeling.\"\"\"\n",
        "    return [\n",
        "        'seller_price', 'seller_badge_encoded', 'should_be_gone', 'seller_pass_rate',\n",
        "        'price_to_earning_ratio', 'seller_products_sold', 'price_per_like', 'brand_id',\n",
        "        'product_type', 'product_material', 'product_like_count', 'seller_num_products_listed',\n",
        "        'seller_community_rank', 'seller_activity_ratio', 'product_color_encoded',\n",
        "        'seller_num_followers', 'available', 'seller_country', 'in_stock',\n",
        "        'product_season_encoded', 'usually_ships_within_encoded', 'product_condition_encoded',\n",
        "        'warehouse_name_encoded'\n",
        "    ]\n",
        "\n",
        "# Step 1: Read the data\n",
        "import requests\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow.fs\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"Reading data from SAS URL...\")\n",
        "\n",
        "# If the SAS URL points to a container, you need to specify a file within it\n",
        "# SAS URLs can be tricky, let's try multiple approaches:\n",
        "\n",
        "# Method 1: Try direct read with storage_options\n",
        "try:\n",
        "    df = pd.read_parquet(sas_url)\n",
        "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "except:\n",
        "    # Method 2: Try with requests\n",
        "    try:\n",
        "        response = requests.get(sas_url)\n",
        "        response.raise_for_status()\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "    except Exception as e:\n",
        "        # Method 3: Try with azure storage specific options\n",
        "        try:\n",
        "            from azure.storage.blob import BlobServiceClient\n",
        "            \n",
        "            # Extract account_url and sas_token from the URL\n",
        "            parts = sas_url.split('?')\n",
        "            if len(parts) == 2:\n",
        "                blob_url = parts[0]\n",
        "                sas_token = '?' + parts[1]\n",
        "                \n",
        "                # Try to list blobs if it's a container URL\n",
        "                print(\"Attempting to list files in the container...\")\n",
        "                blob_service_client = BlobServiceClient(blob_url.split('/')[2], sas_token)\n",
        "                container_name = blob_url.split('/')[-1]\n",
        "                container_client = blob_service_client.get_container_client(container_name)\n",
        "                \n",
        "                # List first 5 files\n",
        "                blob_list = container_client.list_blobs()\n",
        "                files = []\n",
        "                for blob in blob_list:\n",
        "                    files.append(blob.name)\n",
        "                    if len(files) >= 5:\n",
        "                        break\n",
        "                \n",
        "                print(\"First 5 files in container:\")\n",
        "                for f in files:\n",
        "                    print(f\"  - {f}\")\n",
        "                \n",
        "                # Try reading the first parquet file\n",
        "                if files:\n",
        "                    first_file = files[0]\n",
        "                    full_url = f\"{blob_url}/{first_file}{sas_token}\"\n",
        "                    df = pd.read_parquet(full_url)\n",
        "                    print(f\"Data loaded successfully from {first_file}. Shape: {df.shape}\")\n",
        "                    print(f\"Columns: {df.columns.tolist()}\")\n",
        "                else:\n",
        "                    print(\"No files found in container\")\n",
        "            else:\n",
        "                print(\"Invalid SAS URL format\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Error: {e2}\")\n",
        "            print(\"Please provide a valid SAS URL to a specific parquet file\")\n",
        "            # Try to give more context about the error\n",
        "            print(\"\\nYour SAS URL should point to:\")\n",
        "            print(\"1. A specific parquet file (preferred)\")\n",
        "            print(\"2. Or a container with read permissions\")\n",
        "\n",
        "# Step 2: Check if the data has the target variable (sold)\n",
        "if 'sold' in df.columns:\n",
        "    print(\"\\nTraining a model...\")\n",
        "    \n",
        "    # Get features\n",
        "    features = get_important_features()\n",
        "    \n",
        "    # Check which features exist\n",
        "    available_features = [f for f in features if f in df.columns]\n",
        "    missing_features = [f for f in features if f not in df.columns]\n",
        "    \n",
        "    print(f\"Available features: {len(available_features)}/{len(features)}\")\n",
        "    if missing_features:\n",
        "        print(f\"Missing features: {missing_features}\")\n",
        "    \n",
        "    # Prepare data with available features\n",
        "    X = df[available_features]\n",
        "    y = df['sold']\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Apply SMOTE-Tomek\n",
        "    smote_tomek = SMOTETomek(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
        "    \n",
        "    # Train XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        eval_metric='auc',\n",
        "        random_state=42,\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    \n",
        "    # Evaluate\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    joblib.dump(model, 'models/model.pkl')\n",
        "    joblib.dump(available_features, 'models/feature_names.pkl')\n",
        "    print(\"Model saved to 'models/' directory\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\nNo 'sold' column found. Loading model for prediction...\")\n",
        "    \n",
        "    # Load model if it exists\n",
        "    if os.path.exists('models/model.pkl'):\n",
        "        model = joblib.load('models/model.pkl')\n",
        "        feature_names = joblib.load('models/feature_names.pkl')\n",
        "        \n",
        "        # Prepare features\n",
        "        X = df[feature_names]\n",
        "        \n",
        "        # Make predictions\n",
        "        print(\"Making predictions...\")\n",
        "        predictions = model.predict_proba(X)[:, 1]\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results_df = pd.DataFrame({\n",
        "            'id': df.index,\n",
        "            'prediction': predictions\n",
        "        })\n",
        "        \n",
        "        # Save results\n",
        "        results_df.to_csv('predictions.csv', index=False)\n",
        "        print(f\"Predictions saved to 'predictions.csv'\")\n",
        "        print(f\"Number of predictions: {len(results_df)}\")\n",
        "        print(\"\\nSample predictions:\")\n",
        "        print(results_df.head())\n",
        "        \n",
        "    else:\n",
        "        print(\"No model found. Please train a model first.\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Reading data from SAS URL...\nData loaded successfully. Shape: (899281, 36)\nColumns: ['product_id', 'product_type', 'product_name', 'product_description', 'product_keywords', 'product_like_count', 'sold', 'reserved', 'available', 'in_stock', 'should_be_gone', 'brand_id', 'brand_name', 'brand_url', 'product_material', 'product_color', 'price_usd', 'seller_price', 'seller_earning', 'buyers_fees', 'seller_id', 'seller_username', 'seller_country', 'seller_products_sold', 'seller_num_products_listed', 'seller_community_rank', 'seller_num_followers', 'seller_pass_rate', 'product_category_encoded', 'product_season_encoded', 'product_condition_encoded', 'seller_badge_encoded', 'warehouse_name_encoded', 'usually_ships_within_encoded', 'has_cross_border_fees_encoded', 'product_color_encoded']\n\nTraining a model...\nAvailable features: 20/23\nMissing features: ['price_to_earning_ratio', 'price_per_like', 'seller_activity_ratio']\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1746409449182
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}