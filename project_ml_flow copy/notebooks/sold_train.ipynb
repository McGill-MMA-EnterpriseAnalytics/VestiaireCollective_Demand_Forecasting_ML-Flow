{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a7da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet('../data/cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9745a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['margin_rate'] = (df['seller_price'] - df['seller_earning']) / df['seller_price']\n",
    "df.drop(columns=['product_color', 'product_id'], inplace=True)\n",
    "df.drop(columns=['product_category_encoded','has_cross_border_fees_encoded'], inplace=True)\n",
    "\n",
    "df1 = df\n",
    "# Create Derived Features\n",
    "df1['price_to_earning_ratio'] = df1['price_usd'] / (df1['seller_earning'] + 1)  # Avoid division by zero\n",
    "df1['price_per_like'] = df1['price_usd'] / (df1['product_like_count'] + 1)      # Avoid division by zero\n",
    "df1['seller_activity_ratio'] = df1['seller_products_sold'] / (df1['seller_num_products_listed'] + 1)  # Avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28711dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 01:35:45,831] A new study created in memory with name: no-name-bcf7db87-57c6-4da2-9155-b1f11514ec40\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:46,969] Trial 0 finished with value: 0.8541265584860414 and parameters: {'n_estimators': 90, 'max_depth': 8, 'learning_rate': 0.0014334322255991764, 'subsample': 0.7437716157508607, 'colsample_bytree': 0.8286175946046745}. Best is trial 0 with value: 0.8541265584860414.\n",
      "[I 2025-04-29 01:35:46,969] Trial 0 finished with value: 0.8541265584860414 and parameters: {'n_estimators': 90, 'max_depth': 8, 'learning_rate': 0.0014334322255991764, 'subsample': 0.7437716157508607, 'colsample_bytree': 0.8286175946046745}. Best is trial 0 with value: 0.8541265584860414.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:47,974] Trial 1 finished with value: 0.8742604197672849 and parameters: {'n_estimators': 74, 'max_depth': 8, 'learning_rate': 0.03592351451850675, 'subsample': 0.7548099824015837, 'colsample_bytree': 0.9373233123857396}. Best is trial 1 with value: 0.8742604197672849.\n",
      "[I 2025-04-29 01:35:47,974] Trial 1 finished with value: 0.8742604197672849 and parameters: {'n_estimators': 74, 'max_depth': 8, 'learning_rate': 0.03592351451850675, 'subsample': 0.7548099824015837, 'colsample_bytree': 0.9373233123857396}. Best is trial 1 with value: 0.8742604197672849.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:48,734] Trial 2 finished with value: 0.8503945562326742 and parameters: {'n_estimators': 85, 'max_depth': 4, 'learning_rate': 0.023853706553234183, 'subsample': 0.9185806273927055, 'colsample_bytree': 0.7622406944526506}. Best is trial 1 with value: 0.8742604197672849.\n",
      "[I 2025-04-29 01:35:48,734] Trial 2 finished with value: 0.8503945562326742 and parameters: {'n_estimators': 85, 'max_depth': 4, 'learning_rate': 0.023853706553234183, 'subsample': 0.9185806273927055, 'colsample_bytree': 0.7622406944526506}. Best is trial 1 with value: 0.8742604197672849.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:49,231] Trial 3 finished with value: 0.831873541738729 and parameters: {'n_estimators': 33, 'max_depth': 5, 'learning_rate': 0.001328933361461912, 'subsample': 0.7591621806116741, 'colsample_bytree': 0.7424084359769332}. Best is trial 1 with value: 0.8742604197672849.\n",
      "[I 2025-04-29 01:35:49,231] Trial 3 finished with value: 0.831873541738729 and parameters: {'n_estimators': 33, 'max_depth': 5, 'learning_rate': 0.001328933361461912, 'subsample': 0.7591621806116741, 'colsample_bytree': 0.7424084359769332}. Best is trial 1 with value: 0.8742604197672849.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:50,013] Trial 4 finished with value: 0.8747772579855022 and parameters: {'n_estimators': 81, 'max_depth': 4, 'learning_rate': 0.08795459757302326, 'subsample': 0.923193047074617, 'colsample_bytree': 0.7021665258487821}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:50,013] Trial 4 finished with value: 0.8747772579855022 and parameters: {'n_estimators': 81, 'max_depth': 4, 'learning_rate': 0.08795459757302326, 'subsample': 0.923193047074617, 'colsample_bytree': 0.7021665258487821}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:50,482] Trial 5 finished with value: 0.8599189129531881 and parameters: {'n_estimators': 20, 'max_depth': 6, 'learning_rate': 0.06586393910463355, 'subsample': 0.6005957742877044, 'colsample_bytree': 0.7287133402733219}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:50,482] Trial 5 finished with value: 0.8599189129531881 and parameters: {'n_estimators': 20, 'max_depth': 6, 'learning_rate': 0.06586393910463355, 'subsample': 0.6005957742877044, 'colsample_bytree': 0.7287133402733219}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:51,097] Trial 6 finished with value: 0.8503310478065759 and parameters: {'n_estimators': 37, 'max_depth': 7, 'learning_rate': 0.009933069883496647, 'subsample': 0.5741295730177368, 'colsample_bytree': 0.7667258336826526}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:51,097] Trial 6 finished with value: 0.8503310478065759 and parameters: {'n_estimators': 37, 'max_depth': 7, 'learning_rate': 0.009933069883496647, 'subsample': 0.5741295730177368, 'colsample_bytree': 0.7667258336826526}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:51,723] Trial 7 finished with value: 0.8396405388821859 and parameters: {'n_estimators': 25, 'max_depth': 7, 'learning_rate': 0.0019385893630994415, 'subsample': 0.62849799246491, 'colsample_bytree': 0.9215894109640637}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:51,723] Trial 7 finished with value: 0.8396405388821859 and parameters: {'n_estimators': 25, 'max_depth': 7, 'learning_rate': 0.0019385893630994415, 'subsample': 0.62849799246491, 'colsample_bytree': 0.9215894109640637}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:52,429] Trial 8 finished with value: 0.8604626231956115 and parameters: {'n_estimators': 47, 'max_depth': 6, 'learning_rate': 0.03674693802045031, 'subsample': 0.5864368220642728, 'colsample_bytree': 0.9556866117317414}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:52,429] Trial 8 finished with value: 0.8604626231956115 and parameters: {'n_estimators': 47, 'max_depth': 6, 'learning_rate': 0.03674693802045031, 'subsample': 0.5864368220642728, 'colsample_bytree': 0.9556866117317414}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:52,815] Trial 9 finished with value: 0.8476604505382079 and parameters: {'n_estimators': 16, 'max_depth': 7, 'learning_rate': 0.004400809526188971, 'subsample': 0.9801641905244223, 'colsample_bytree': 0.5871298718784127}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:52,815] Trial 9 finished with value: 0.8476604505382079 and parameters: {'n_estimators': 16, 'max_depth': 7, 'learning_rate': 0.004400809526188971, 'subsample': 0.9801641905244223, 'colsample_bytree': 0.5871298718784127}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:53,441] Trial 10 finished with value: 0.8645493864179266 and parameters: {'n_estimators': 68, 'max_depth': 3, 'learning_rate': 0.09056342912937348, 'subsample': 0.8748178322955508, 'colsample_bytree': 0.5048174622346053}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:53,441] Trial 10 finished with value: 0.8645493864179266 and parameters: {'n_estimators': 68, 'max_depth': 3, 'learning_rate': 0.09056342912937348, 'subsample': 0.8748178322955508, 'colsample_bytree': 0.5048174622346053}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:54,801] Trial 11 finished with value: 0.8718707168071915 and parameters: {'n_estimators': 69, 'max_depth': 10, 'learning_rate': 0.023840023999449275, 'subsample': 0.732574238901448, 'colsample_bytree': 0.6233621254729553}. Best is trial 4 with value: 0.8747772579855022.\n",
      "[I 2025-04-29 01:35:54,801] Trial 11 finished with value: 0.8718707168071915 and parameters: {'n_estimators': 69, 'max_depth': 10, 'learning_rate': 0.023840023999449275, 'subsample': 0.732574238901448, 'colsample_bytree': 0.6233621254729553}. Best is trial 4 with value: 0.8747772579855022.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:55,966] Trial 12 finished with value: 0.8788791873991242 and parameters: {'n_estimators': 73, 'max_depth': 9, 'learning_rate': 0.048135352666613875, 'subsample': 0.8335709913993804, 'colsample_bytree': 0.8679654966697854}. Best is trial 12 with value: 0.8788791873991242.\n",
      "[I 2025-04-29 01:35:55,966] Trial 12 finished with value: 0.8788791873991242 and parameters: {'n_estimators': 73, 'max_depth': 9, 'learning_rate': 0.048135352666613875, 'subsample': 0.8335709913993804, 'colsample_bytree': 0.8679654966697854}. Best is trial 12 with value: 0.8788791873991242.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:57,889] Trial 13 finished with value: 0.8831523874234632 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.09828229489799648, 'subsample': 0.8612854119414836, 'colsample_bytree': 0.8507221386184016}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:35:57,889] Trial 13 finished with value: 0.8831523874234632 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.09828229489799648, 'subsample': 0.8612854119414836, 'colsample_bytree': 0.8507221386184016}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:35:59,651] Trial 14 finished with value: 0.8677991540371579 and parameters: {'n_estimators': 96, 'max_depth': 10, 'learning_rate': 0.014570201609439495, 'subsample': 0.865124713088037, 'colsample_bytree': 0.8602239142402393}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:35:59,651] Trial 14 finished with value: 0.8677991540371579 and parameters: {'n_estimators': 96, 'max_depth': 10, 'learning_rate': 0.014570201609439495, 'subsample': 0.865124713088037, 'colsample_bytree': 0.8602239142402393}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:36:00,651] Trial 15 finished with value: 0.8729697517623388 and parameters: {'n_estimators': 56, 'max_depth': 9, 'learning_rate': 0.0463643627353074, 'subsample': 0.8048884494700126, 'colsample_bytree': 0.878344424805004}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:36:00,651] Trial 15 finished with value: 0.8729697517623388 and parameters: {'n_estimators': 56, 'max_depth': 9, 'learning_rate': 0.0463643627353074, 'subsample': 0.8048884494700126, 'colsample_bytree': 0.878344424805004}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:36:02,393] Trial 16 finished with value: 0.863224879990659 and parameters: {'n_estimators': 98, 'max_depth': 9, 'learning_rate': 0.006831700480434183, 'subsample': 0.6695635673744129, 'colsample_bytree': 0.8097399579710384}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:36:02,393] Trial 16 finished with value: 0.863224879990659 and parameters: {'n_estimators': 98, 'max_depth': 9, 'learning_rate': 0.006831700480434183, 'subsample': 0.6695635673744129, 'colsample_bytree': 0.8097399579710384}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:36:03,343] Trial 17 finished with value: 0.8764289452973753 and parameters: {'n_estimators': 57, 'max_depth': 9, 'learning_rate': 0.0548067011277421, 'subsample': 0.8364018003212218, 'colsample_bytree': 0.8938875718018671}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:36:03,343] Trial 17 finished with value: 0.8764289452973753 and parameters: {'n_estimators': 57, 'max_depth': 9, 'learning_rate': 0.0548067011277421, 'subsample': 0.8364018003212218, 'colsample_bytree': 0.8938875718018671}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:36:04,698] Trial 18 finished with value: 0.8772992083065412 and parameters: {'n_estimators': 78, 'max_depth': 10, 'learning_rate': 0.09782805220372996, 'subsample': 0.5041757986845241, 'colsample_bytree': 0.9606876325393422}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:36:04,698] Trial 18 finished with value: 0.8772992083065412 and parameters: {'n_estimators': 78, 'max_depth': 10, 'learning_rate': 0.09782805220372996, 'subsample': 0.5041757986845241, 'colsample_bytree': 0.9606876325393422}. Best is trial 13 with value: 0.8831523874234632.\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_30448\\4054299697.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
      "[I 2025-04-29 01:36:05,657] Trial 19 finished with value: 0.8662337586993182 and parameters: {'n_estimators': 65, 'max_depth': 8, 'learning_rate': 0.021268594894599067, 'subsample': 0.9976035852268088, 'colsample_bytree': 0.6726368693696444}. Best is trial 13 with value: 0.8831523874234632.\n",
      "[I 2025-04-29 01:36:05,657] Trial 19 finished with value: 0.8662337586993182 and parameters: {'n_estimators': 65, 'max_depth': 8, 'learning_rate': 0.021268594894599067, 'subsample': 0.9976035852268088, 'colsample_bytree': 0.6726368693696444}. Best is trial 13 with value: 0.8831523874234632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation ROC-AUC: 0.8832 | Time: 0.03 min\n",
      "XGBoost Learning Curve completed in 0.15 min\n",
      "XGBoost Learning Curve completed in 0.15 min\n",
      "[LightGBM] [Info] Number of positive: 2471, number of negative: 159399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3317\n",
      "[LightGBM] [Info] Number of data points in the train set: 161870, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015265 -> initscore=-4.166788\n",
      "[LightGBM] [Info] Start training from score -4.166788\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2471, number of negative: 159399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3317\n",
      "[LightGBM] [Info] Number of data points in the train set: 161870, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015265 -> initscore=-4.166788\n",
      "[LightGBM] [Info] Start training from score -4.166788\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Validation ROC-AUC: 0.8553 | Time: 0.01 min\n",
      "LightGBM Validation ROC-AUC: 0.8553 | Time: 0.01 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abmir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-workflow-88owM936-py3.11\\Lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Learning Curve completed in 0.08 min\n",
      "CatBoost Validation ROC-AUC: 0.8684 | Time: 0.01 min\n",
      "CatBoost Validation ROC-AUC: 0.8684 | Time: 0.01 min\n",
      "CatBoost Learning Curve completed in 0.10 min\n",
      "\n",
      "Logging Final Performance on Test Set to MLflow:\n",
      "CatBoost Learning Curve completed in 0.10 min\n",
      "\n",
      "Logging Final Performance on Test Set to MLflow:\n",
      "{'XGBoost_test_roc_auc': 0.8907427636246119, 'XGBoost_test_accuracy': 0.9872491057694089, 'XGBoost_test_precision': 0.9245283018867925, 'XGBoost_test_recall': 0.17861482381530985, 'XGBoost_test_f1': 0.29938900203665986, 'XGBoost_test_log_loss': 0.05411891426214484}\n",
      "{'XGBoost_test_roc_auc': 0.8907427636246119, 'XGBoost_test_accuracy': 0.9872491057694089, 'XGBoost_test_precision': 0.9245283018867925, 'XGBoost_test_recall': 0.17861482381530985, 'XGBoost_test_f1': 0.29938900203665986, 'XGBoost_test_log_loss': 0.05411891426214484}\n",
      "{'LightGBM_test_roc_auc': 0.8605019515298695, 'LightGBM_test_accuracy': 0.9855811108845933, 'LightGBM_test_precision': 0.7848101265822784, 'LightGBM_test_recall': 0.07533414337788578, 'LightGBM_test_f1': 0.13747228381374724, 'LightGBM_test_log_loss': 0.06121900291843374}\n",
      "{'LightGBM_test_roc_auc': 0.8605019515298695, 'LightGBM_test_accuracy': 0.9855811108845933, 'LightGBM_test_precision': 0.7848101265822784, 'LightGBM_test_recall': 0.07533414337788578, 'LightGBM_test_f1': 0.13747228381374724, 'LightGBM_test_log_loss': 0.06121900291843374}\n",
      "{'CatBoost_test_roc_auc': 0.8699588756110836, 'CatBoost_test_accuracy': 0.98760123802287, 'CatBoost_test_precision': 0.9375, 'CatBoost_test_recall': 0.20048602673147023, 'CatBoost_test_f1': 0.3303303303303303, 'CatBoost_test_log_loss': 0.05570689128011065}\n",
      "{'CatBoost_test_roc_auc': 0.8699588756110836, 'CatBoost_test_accuracy': 0.98760123802287, 'CatBoost_test_precision': 0.9375, 'CatBoost_test_recall': 0.20048602673147023, 'CatBoost_test_f1': 0.3303303303303303, 'CatBoost_test_log_loss': 0.05570689128011065}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, learning_curve, StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, log_loss, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import shap\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Set MLflow experiment to save logs in default './mlruns/' folder\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"Vestiaire_Model_Comparison\")\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Filter dataset to the most important features\n",
    "important_features = [\n",
    "    'seller_price', 'seller_badge_encoded', 'should_be_gone', 'seller_pass_rate',\n",
    "    'price_to_earning_ratio', 'seller_products_sold', 'price_per_like', 'brand_id',\n",
    "    'product_type', 'product_material', 'product_like_count', 'seller_num_products_listed',\n",
    "    'seller_community_rank', 'seller_activity_ratio', 'product_color_encoded',\n",
    "    'seller_num_followers', 'margin_rate', 'available', 'seller_country', 'in_stock',\n",
    "    'product_season_encoded', 'usually_ships_within_encoded', 'product_condition_encoded',\n",
    "    'warehouse_name_encoded'\n",
    "]\n",
    "\n",
    "# Assuming your DataFrame is named 'df1'\n",
    "X = df1[important_features]\n",
    "y = df1['sold']\n",
    "\n",
    "# Use a sample of the dataset for faster experimentation (30% of data)\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X, y, train_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split sample into train (60%), validation (20%), and test (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.4, random_state=42, stratify=y_sample\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ─── BAYESIAN HYPERPARAMETER TUNING WITH OPTUNA ────────────────────────────────\n",
    "with mlflow.start_run(run_name=\"Hyperparameter_Tuning\") as tuning_run:\n",
    "    parent_run_id = tuning_run.info.run_id\n",
    "\n",
    "    def objective(trial):\n",
    "        # sample hyperparameters\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100),\n",
    "            \"max_depth\":    trial.suggest_int(\"max_depth\",  3,  10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "            \"subsample\":    trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\",\n",
    "                              nested=True,\n",
    "                              parent_run_id=parent_run_id):\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # train & eval on validation set\n",
    "            model = xgb.XGBClassifier(\n",
    "                **params,\n",
    "                eval_metric='auc',\n",
    "                random_state=42,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "            auc = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "            mlflow.log_metric(\"validation_roc_auc\", auc)\n",
    "\n",
    "        return auc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    # log best trial\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_validation_roc_auc\", study.best_value)\n",
    "\n",
    "    # optionally retrieve best params for downstream training\n",
    "    best_params_xgb = study.best_params\n",
    "\n",
    "# ─── MODEL INITIALIZATION ─────────────────────────────────────────────────────\n",
    "# use the tuned params for XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_params_xgb,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "# keep your LightGBM & CatBoost defaults or tune separately\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=30, max_depth=3, device='cpu')\n",
    "cat_model = cb.CatBoostClassifier(verbose=0, random_state=42, iterations=30, depth=3, task_type='CPU')\n",
    "\n",
    "def train_and_evaluate_with_mlflow(model, model_name, X_train, y_train, X_val, y_val, parent_run_id=None):\n",
    "    with mlflow.start_run(run_name=model_name, nested=True, parent_run_id=parent_run_id):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model on the training set\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Calculate metrics on validation set\n",
    "        roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        logloss = log_loss(y_val, y_pred_prob)\n",
    "\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        print(f\"{model_name} Validation ROC-AUC: {roc_auc:.4f} | Time: {elapsed_time:.2f} min\")\n",
    "\n",
    "        # Log parameters and validation metrics\n",
    "        mlflow.log_params(model.get_params())\n",
    "        mlflow.log_metrics({\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"log_loss\": logloss\n",
    "        })\n",
    "\n",
    "        # Log ROC & PR curves as artifacts\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "        plt.figure(); plt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.2f})\"); plt.plot([0,1],[0,1],'k--')\n",
    "        plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f\"{model_name} ROC (Validation)\"); plt.legend()\n",
    "        roc_path = os.path.join(temp_dir, f\"{model_name}_validation_roc.png\")\n",
    "        plt.savefig(roc_path); mlflow.log_artifact(roc_path); plt.close()\n",
    "\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "        plt.figure(); plt.plot(recall_vals, precision_vals, label=\"PR Curve\")\n",
    "        plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f\"{model_name} PR (Validation)\"); plt.legend()\n",
    "        prc_path = os.path.join(temp_dir, f\"{model_name}_validation_prc.png\")\n",
    "        plt.savefig(prc_path); mlflow.log_artifact(prc_path); plt.close()\n",
    "\n",
    "        # ─── EXTRA ARTIFACTS ────────────────────────────────────────────────────\n",
    "\n",
    "        # 1. Confusion Matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        plt.figure(); plt.imshow(cm); plt.title(f\"{model_name} Confusion Matrix\")\n",
    "        plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "        cm_path = os.path.join(temp_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "        plt.savefig(cm_path); mlflow.log_artifact(cm_path); plt.close()\n",
    "\n",
    "        # 2. SHAP Feature Importance Summary Plot\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_val)\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_val, show=False)\n",
    "        shap_path = os.path.join(temp_dir, f\"{model_name}_shap_summary.png\")\n",
    "        plt.savefig(shap_path); mlflow.log_artifact(shap_path); plt.close()\n",
    "\n",
    "        # 3. Sample Predictions CSV\n",
    "        sample_df = X_val.copy()\n",
    "        sample_df['actual'] = y_val\n",
    "        sample_df['predicted'] = y_pred\n",
    "        sample_df['pred_proba'] = y_pred_prob\n",
    "        sample_csv = os.path.join(temp_dir, f\"{model_name}_sample_predictions.csv\")\n",
    "        sample_df.head(20).to_csv(sample_csv, index=False)\n",
    "        mlflow.log_artifact(sample_csv)\n",
    "\n",
    "        return model\n",
    "    \n",
    "def plot_learning_curve_and_log(model, X_train, y_train, model_name):\n",
    "    start_time = time.time()\n",
    "    cv = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=42)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 0.5, 3)\n",
    "    )\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_sizes, train_mean, label=f\"{model_name} Train\")\n",
    "    plt.plot(train_sizes, test_mean, label=f\"{model_name} Validation\")\n",
    "    plt.title(f\"{model_name} Learning Curve\")\n",
    "    plt.xlabel(\"Training Size\"); plt.ylabel(\"ROC-AUC Score\"); plt.legend(); plt.grid()\n",
    "\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    plot_path = os.path.join(temp_dir, f\"{model_name}_learning_curve.png\")\n",
    "    plt.savefig(plot_path); mlflow.log_artifact(plot_path); plt.close()\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    print(f\"{model_name} Learning Curve completed in {elapsed_time:.2f} min\")\n",
    "\n",
    "# Parent run to group all child runs\n",
    "with mlflow.start_run(run_name=\"Model Comparison\") as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "\n",
    "    # Validation experiments\n",
    "    xgb_model = train_and_evaluate_with_mlflow(xgb_model, \"XGBoost\", X_train, y_train, X_val, y_val, parent_run_id)\n",
    "    plot_learning_curve_and_log(xgb_model, X_train, y_train, \"XGBoost\")\n",
    "\n",
    "    lgb_model = train_and_evaluate_with_mlflow(lgb_model, \"LightGBM\", X_train, y_train, X_val, y_val, parent_run_id)\n",
    "    plot_learning_curve_and_log(lgb_model, X_train, y_train, \"LightGBM\")\n",
    "\n",
    "    cat_model = train_and_evaluate_with_mlflow(cat_model, \"CatBoost\", X_train, y_train, X_val, y_val, parent_run_id)\n",
    "    plot_learning_curve_and_log(cat_model, X_train, y_train, \"CatBoost\")\n",
    "\n",
    "    # Final evaluation on the held-out test set\n",
    "    with mlflow.start_run(run_name=\"Test_Evaluation\", nested=True, parent_run_id=parent_run_id):\n",
    "        print(\"\\nLogging Final Performance on Test Set to MLflow:\")\n",
    "        for model, model_name in [(xgb_model, \"XGBoost\"), (lgb_model, \"LightGBM\"), (cat_model, \"CatBoost\")]:\n",
    "            y_test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            metrics = {\n",
    "                f\"{model_name}_test_roc_auc\": roc_auc_score(y_test, y_test_pred_prob),\n",
    "                f\"{model_name}_test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "                f\"{model_name}_test_precision\": precision_score(y_test, y_test_pred),\n",
    "                f\"{model_name}_test_recall\": recall_score(y_test, y_test_pred),\n",
    "                f\"{model_name}_test_f1\": f1_score(y_test, y_test_pred),\n",
    "                f\"{model_name}_test_log_loss\": log_loss(y_test, y_test_pred_prob)\n",
    "            }\n",
    "            print(metrics)\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            # Log test ROC and PR curves\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_test_pred_prob)\n",
    "            plt.figure(); plt.plot(fpr, tpr, label=f\"ROC (AUC={metrics[f'{model_name}_test_roc_auc']:.2f})\"); plt.plot([0,1],[0,1],'k--')\n",
    "            plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f\"{model_name} ROC (Test)\"); plt.legend()\n",
    "            roc_path = os.path.join(temp_dir, f\"{model_name}_test_roc.png\")\n",
    "            plt.savefig(roc_path); mlflow.log_artifact(roc_path); plt.close()\n",
    "\n",
    "            precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_pred_prob)\n",
    "            plt.figure(); plt.plot(recall_vals, precision_vals, label=\"PR Curve\"); plt.xlabel('Recall'); plt.ylabel('Precision');\n",
    "            plt.title(f\"{model_name} PR (Test)\"); plt.legend()\n",
    "            prc_path = os.path.join(temp_dir, f\"{model_name}_test_prc.png\")\n",
    "            plt.savefig(prc_path); mlflow.log_artifact(prc_path); plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry (ml-workflow)",
   "language": "python",
   "name": "ml-workflow-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
